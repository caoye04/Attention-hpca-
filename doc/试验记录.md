



| Size     | Version1_naive_GPU | Version2\_编译优化\_GPU | Version3\_共享内存优化\_GPU | Version4_Warp级别Softmax计算优化_GPU |
| -------- | ------------------ | ----------------------- | --------------------------- | ------------------------------------ |
| 63       | 7.63               | 8.09                    | 7.89                        | 8.16                                 |
| 64       | 7.82               | 8.13                    | 8.09                        | 8.39                                 |
| 65       | 8.42               | 8.93                    | 8.48                        | 8.95                                 |
| 127      | 49.4               | 51.6                    | 49.8                        | 52.3                                 |
| 128      | 45.4               | 46.2                    | 46.0                        | 48.4                                 |
| 129      | 51.8               | 53.6                    | 50.5                        | 54.0                                 |
| 191      | 119                | 120                     | 120                         | 124                                  |
| 192      | 88.6               | 93.8                    | 98.8                        | 102                                  |
| 193      | 120                | 120                     | 121                         | 124                                  |
| 255      | 196                | 200                     | 214                         | 211                                  |
| 256      | 146                | 148                     | 177                         | 176                                  |
| 257      | 199                | 202                     | 214                         | 213                                  |
| 319      | 284                | 283                     | 303                         | 321                                  |
| 320      | 191                | 198                     | 230                         | 241                                  |
| 321      | 285                | 284                     | 303                         | 318                                  |
| 383      | 409                | 410                     | 472                         | 479                                  |
| 384      | 272                | 279                     | 361                         | 366                                  |
| 385      | 398                | 399                     | 433                         | 469                                  |
| 447      | 545                | 553                     | 621                         | 646                                  |
| 448      | 351                | 357                     | 499                         | 521                                  |
| 449      | 546                | 552                     | 615                         | 633                                  |
| 511      | 678                | 691                     | 768                         | 813                                  |
| 512      | 418                | 423                     | 630                         | 667                                  |
| 513      | 682                | 694                     | 755                         | 809                                  |
| 575      | 742                | 747                     | 947                         | 955                                  |
| 576      | 450                | 458                     | 745                         | 752                                  |
| 577      | 741                | 745                     | 870                         | 938                                  |
| 639      | 736                | 734                     | 1040                        | 1060                                 |
| 640      | 468                | 476                     | 805                         | 817                                  |
| 641      | 735                | 733                     | 966                         | 1040                                 |
| 703      | 943                | 953                     | 1240                        | 1260                                 |
| 704      | 547                | 552                     | 1020                        | 1040                                 |
| 705      | 936                | 946                     | 1140                        | 1230                                 |
| 767      | 904                | 905                     | 1300                        | 1320                                 |
| 768      | 555                | 562                     | 1040                        | 1070                                 |
| 769      | 898                | 898                     | 1210                        | 1300                                 |
| 831      | 1040               | 1050                    | 1390                        | 1470                                 |
| 832      | 596                | 602                     | 1140                        | 1210                                 |
| 833      | 1030               | 1040                    | 1360                        | 1440                                 |
| 895      | 1130               | 1140                    | 1490                        | 1600                                 |
| 896      | 633                | 637                     | 1270                        | 1350                                 |
| 897      | 1120               | 1130                    | 1450                        | 1570                                 |
| 959      | 1020               | 1010                    | 1570                        | 1660                                 |
| 960      | 627                | 634                     | 1290                        | 1350                                 |
| 961      | 1010               | 1000                    | 1510                        | 1620                                 |
| 1023     | 1310               | 1320                    | 1730                        | 1820                                 |
| 1024     | 686                | 689                     | 1480                        | 1570                                 |
| 1025     | 1270               | 1280                    | 1650                        | 1730                                 |
| 1087     | 1120               | 1120                    | 1760                        | 1820                                 |
| 1088     | 665                | 669                     | 1470                        | 1530                                 |
| 1089     | 1120               | 1120                    | 1680                        | 1800                                 |
| 1151     | 1240               | 1250                    | 1810                        | 1940                                 |
| 1152     | 695                | 700                     | 1550                        | 1650                                 |
| 1153     | 1230               | 1230                    | 1730                        | 1900                                 |
| 1215     | 1290               | 1300                    | 1830                        | 2030                                 |
| 1216     | 712                | 715                     | 1600                        | 1760                                 |
| 1217     | 1280               | 1290                    | 1750                        | 1980                                 |
| 1279     | 1040               | 1080                    | 1790                        | 2000                                 |
| 1280     | 687                | 692                     | 1540                        | 1710                                 |
| 1281     | 1040               | 1080                    | 1730                        | 1960                                 |
| 1343     | 1180               | 1250                    | 1940                        | 2110                                 |
| 1344     | 713                | 718                     | 1670                        | 1830                                 |
| 1345     | 1170               | 1240                    | 1810                        | 2060                                 |
| 1407     | 1270               | 1360                    | 1950                        | 2180                                 |
| 1408     | 733                | 736                     | 1720                        | 1930                                 |
| 1409     | 1250               | 1350                    | 1880                        | 2140                                 |
| 1471     | 1240               | 1320                    | 1980                        | 2210                                 |
| 1472     | 739                | 741                     | 1750                        | 1970                                 |
| 1473     | 1230               | 1310                    | 1910                        | 2150                                 |
| 1535     | 1150               | 1220                    | 2010                        | 2210                                 |
| 1536     | 733                | 737                     | 1770                        | 1960                                 |
| 1537     | 1140               | 1210                    | 1950                        | 2170                                 |
| 1599     | 1180               | 1250                    | 2060                        | 2240                                 |
| 1600     | 732                | 737                     | 1800                        | 1980                                 |
| 1601     | 1170               | 1240                    | 2000                        | 2200                                 |
| 1663     | 1290               | 1350                    | 2080                        | 2280                                 |
| 1664     | 754                | 756                     | 1840                        | 2070                                 |
| 1665     | 1270               | 1340                    | 2020                        | 2230                                 |
| 1727     | 1260               | 1320                    | 2130                        | 2300                                 |
| 1728     | 756                | 755                     | 1880                        | 2100                                 |
| 1729     | 1250               | 1310                    | 2030                        | 2260                                 |
| 1791     | 1340               | 1400                    | 2150                        | 2300                                 |
| 1792     | 763                | 762                     | 1920                        | 2150                                 |
| 1793     | 1330               | 1380                    | 2100                        | 2280                                 |
| 1855     | 1300               | 1340                    | 2160                        | 2330                                 |
| 1856     | 766                | 765                     | 1930                        | 2170                                 |
| 1857     | 1280               | 1340                    | 2070                        | 2300                                 |
| 1919     | 1160               | 1200                    | 2160                        | 2330                                 |
| 1920     | 751                | 754                     | 1930                        | 2140                                 |
| 1921     | 1150               | 1190                    | 2070                        | 2290                                 |
| 1983     | 1340               | 1390                    | 2230                        | 2370                                 |
| 1984     | 774                | 775                     | 2000                        | 2210                                 |
| 1985     | 1330               | 1380                    | 2140                        | 2340                                 |
| 2047     | 1470               | 1520                    | 2270                        | 2400                                 |
| 2048     | 785                | 784                     | 2040                        | 2240                                 |
| 2049     | 1440               | 1490                    | 2170                        | 2350                                 |
| 4095     | 1560               | 1580                    | 2490                        | 2550                                 |
| 4096     | 803                | 800                     | 2270                        | 2460                                 |
| 4097     | 1540               | 1550                    | 2440                        | 2530                                 |
| 8191     | 1590               | 1590                    | 2550                        | 2500                                 |
| 8192     | 818                | 817                     | 2340                        | 2500                                 |
| 8193     | 1560               | 1570                    | 2520                        | 2500                                 |
| 平均性能 | 837.263590         | 855.904458              | 1514.408617                 | 2032.730586                          |



# 并行计算基础HW—Attention

## 1. 作业背景

Attention 机制最初用于提升序列到序列模型（Seq2Seq）在处理长文本时的效果，通过允许模型动态聚焦输入序列的不同部分，解决了传统方法的瓶颈。它通过计算查询（Q）、键（K）和值（V）之间的相似度来加权每个值，从而生成新的表示。Attention 被广泛应用于 NLP、计算机视觉等领域，尤其是 Transformer 模型中。随着模型应用的扩展，优化 Attention 计算的效率变得尤为重要，常见的优化技巧包括稀疏化计算、低秩近似、多机分布式计算以及 GPU 加速等方法，以减少计算和内存开销，提升效率。

## 2. 作业任务：方阵的 Attention 机制计算

### 2.1. 任务描述

完成 Attention 算子在GPU 单卡上的实现与优化。Attention 算子：

$$
Y=\operatorname{Softmax}\left(\frac{Q K^{\mathrm{T}}}{\sqrt{N}}\right) V
$$


其中，$Q, K, V$ 均为 $N \times N$ 的单精度行优先存储的稠密矩阵。
其中 Softmax 的定义为

$$
\operatorname{Softmax}\left(\left[z_1, \ldots, z_n\right]\right)=\left[\frac{e^{z_1}}{\sum_{j=1}^n e^{z_j}}, \ldots, \frac{e^{z_n}}{\sum_{j=1}^n e^{z_j}}\right]
$$


在 Attention 计算中，Softmax 的计算是逐行进行的，具体算法实现可以参考＂code／cpu／naive．c＂中的代码。在熟悉代码核心后，优化版本的程序实现在＂code／gpu／opt．cu＂中。

### 2.2 正确性验证

1．采用 float 单精度浮点数据类型进行运算，运算结果通过作业基础代码中的正确性验证，eps 为机器的单精度极小值，约为 $10^{-6}$ 左右。

$$
\| \text { custom-attention }(n, Q, K, V)-\text { Label } \|<100 n^2 \epsilon
$$

其中，$\|\cdot\|$ 表示将矩阵逐元素取绝对值求和（即向量的 1 范数）。

2．Attention 的主要计算开销在两次矩阵乘法，其计算复杂度为 $O\left(N^3\right)$ ，在计算性能指标的时候采用 $\left(4 N^3\right)$ 计算，如果采用了一些非 $O\left(N^3\right)$ 算法而导致通过不了正确性测试，这种情况可以适当且合理地放宽精度的要求，但需要在作业报告中指出。

3．开展必要的性能分析，比如某些矩阵规模性能出现明显的降低，可以采用性能分析工具进行性能分析。

## 3. 作业评分

1. 通过正确性检验（ $20 \%$ ）。

2. 评测 Attention 算子在不同输入（共 102 个测例）下的性能结果，按照提交后的性能排序结果，以及代码质量进行打分（ $50 \%$ ）。

3. 详细描述在实现 Attention 算子中采取的优化手段，代码对应的部分，以及对应的实验结果，可以采用性能工具或者模型来解释目前取得的性能结果（ $20 \%$ ）。

4. 给出一张完整的实验结果图，描述当前算法的性能，横坐标为矩阵规模，纵坐标为 Gflop $/ s(10 \%)$ 。

5. 可以参考矩阵乘法相关的外部库（如 BLAS 或 BLIS 等数学库）的实现与优化思路，但禁止直接使用外部库作为作业成果。

> 作业提示：
>
> 1．在保证正确性的前提下，可对计算流程中的某些冗余部分进行删减。
>
> 2．本作业的核心模块是两次矩阵乘法，可以借鉴第一次作业 SGEMM的优化思路，在 GPU 上优化好矩阵运算的子模块。
>
> 3．有任何问题欢迎与助教和老师交流。





# 我的实验：

### version1

```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

const char* version_name = "Optimized implementation.";


__global__ void matmul_QKT(int n, float* Q, float* K, float* QKT, float scale) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < n && col < n) {
        float sum = 0.0f;
        for (int k = 0; k < n; k++) {
            sum += Q[row * n + k] * K[col * n + k]; 
        }
        QKT[row * n + col] = sum * scale;
    }
}

__global__ void softmax_kernel(int n, float* QKT) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < n) {
        float max_val = -INFINITY;
        for (int j = 0; j < n; j++) {
            if (QKT[row * n + j] > max_val) {
                max_val = QKT[row * n + j];
            }
        }
        float sum_exp = 0.0f;
        for (int j = 0; j < n; j++) {
            QKT[row * n + j] = expf(QKT[row * n + j] - max_val);
            sum_exp += QKT[row * n + j];
        }
        
        for (int j = 0; j < n; j++) {
            QKT[row * n + j] /= sum_exp;
        }
    }
}

__global__ void matmul_softmax_V(int n, float* softmax_QKT, float* V, float* Y) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < n && col < n) {
        float sum = 0.0f;
        for (int k = 0; k < n; k++) {
            sum += softmax_QKT[row * n + k] * V[k * n + col];
        }
        Y[row * n + col] = sum;
    }
}

void square_attention(int n, float* gpu_Q, float* gpu_K, float* gpu_V, float* gpu_Y) {

    float* gpu_QKT;
    cudaMalloc(&gpu_QKT, n * n * sizeof(float));
    
    float scale = 1.0f / sqrtf(n);
    
    dim3 block_dim(16, 16);
    dim3 grid_dim((n + block_dim.x - 1) / block_dim.x, 
                  (n + block_dim.y - 1) / block_dim.y);

    dim3 softmax_block_dim(256);
    dim3 softmax_grid_dim((n + softmax_block_dim.x - 1) / softmax_block_dim.x);
    
    matmul_QKT<<<grid_dim, block_dim>>>(n, gpu_Q, gpu_K, gpu_QKT, scale);

    softmax_kernel<<<softmax_grid_dim, softmax_block_dim>>>(n, gpu_QKT);
    
    matmul_softmax_V<<<grid_dim, block_dim>>>(n, gpu_QKT, gpu_V, gpu_Y);
    
    cudaFree(gpu_QKT);
    
    cudaDeviceSynchronize();
}
```



### Version2 编译优化

naive：837.263590

-O3：837.499209 

-use_fast_math：855.904458 

-arch=sm_86：837.628005 

```cmd
(base) [t2021012167@admin gpu]$ srun --gres=gpu:1 ./check_gpu
GPU Name: NVIDIA A40
Compute Capability: 8.6
```



### Version3 ：内存访问优化

naive版本中，实现矩阵乘法直接从全局内存读取数据，这会导致较高的内存延迟。尝试使用共享内存来缓存数据块

TILE_SIZE 8   ：1221.453162 

TILE_SIZE 16 ：1514.408617 

TILE_SIZE 32 ：1402.859883 

```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

const char* version_name = "Shared Memory Optimized implementation.";

#define TILE_SIZE 16

__global__ void matmul_QKT_shared(int n, float* Q, float* K, float* QKT, float scale) {
    __shared__ float tile_Q[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_K[TILE_SIZE][TILE_SIZE];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    for (int tile = 0; tile < (n + TILE_SIZE - 1) / TILE_SIZE; ++tile) {
        if (row < n && tile * TILE_SIZE + tx < n)
            tile_Q[ty][tx] = Q[row * n + tile * TILE_SIZE + tx];
        else
            tile_Q[ty][tx] = 0.0f;
        
        if (col < n && tile * TILE_SIZE + ty < n)
            tile_K[ty][tx] = K[col * n + tile * TILE_SIZE + ty]; 
        else
            tile_K[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += tile_Q[ty][k] * tile_K[k][tx];
        }

        __syncthreads();
    }
    
    if (row < n && col < n)
        QKT[row * n + col] = sum * scale;
}

__global__ void softmax_kernel(int n, float* QKT) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < n) {
        float max_val = -INFINITY;
        for (int j = 0; j < n; j++) {
            if (QKT[row * n + j] > max_val) {
                max_val = QKT[row * n + j];
            }
        }
        float sum_exp = 0.0f;
        for (int j = 0; j < n; j++) {
            QKT[row * n + j] = expf(QKT[row * n + j] - max_val);
            sum_exp += QKT[row * n + j];
        }
        
        for (int j = 0; j < n; j++) {
            QKT[row * n + j] /= sum_exp;
        }
    }
}

__global__ void matmul_softmax_V_shared(int n, float* softmax_QKT, float* V, float* Y) {
    __shared__ float tile_S[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_V[TILE_SIZE][TILE_SIZE];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    for (int tile = 0; tile < (n + TILE_SIZE - 1) / TILE_SIZE; ++tile) {
        if (row < n && tile * TILE_SIZE + tx < n)
            tile_S[ty][tx] = softmax_QKT[row * n + tile * TILE_SIZE + tx];
        else
            tile_S[ty][tx] = 0.0f;
            
        if (tile * TILE_SIZE + ty < n && col < n)
            tile_V[ty][tx] = V[(tile * TILE_SIZE + ty) * n + col];
        else
            tile_V[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += tile_S[ty][k] * tile_V[k][tx];
        }
        
        __syncthreads();
    }
    
    if (row < n && col < n)
        Y[row * n + col] = sum;
}

void square_attention(int n, float* gpu_Q, float* gpu_K, float* gpu_V, float* gpu_Y) {
    float* gpu_QKT;
    cudaMalloc(&gpu_QKT, n * n * sizeof(float));
    
    float scale = 1.0f / sqrtf(n);
    
    dim3 block_dim(TILE_SIZE, TILE_SIZE);
    dim3 grid_dim((n + TILE_SIZE - 1) / TILE_SIZE, 
                  (n + TILE_SIZE - 1) / TILE_SIZE);
    
    dim3 softmax_block_dim(256);
    dim3 softmax_grid_dim((n + softmax_block_dim.x - 1) / softmax_block_dim.x);
    
    matmul_QKT_shared<<<grid_dim, block_dim>>>(n, gpu_Q, gpu_K, gpu_QKT, scale);
    
    softmax_kernel<<<softmax_grid_dim, softmax_block_dim>>>(n, gpu_QKT);
    
    matmul_softmax_V_shared<<<grid_dim, block_dim>>>(n, gpu_QKT, gpu_V, gpu_Y);
    
    cudaFree(gpu_QKT);
    
    cudaDeviceSynchronize();
}
```

### Verion4：Warp级别Softmax优化实现

> 在原始实现中，每个线程负责处理一整行的Softmax计算，这导致：
>
> - 每个线程需要多次遍历矩阵行
> - 线程之间完全独立，没有利用warp内协作
>
> 新版本中：
>
> - 每个warp（32个线程）协作处理一行
> - 行内的元素被warp内的线程平均分配
> - 通过线程协作，减少了全局内存的遍历次数
>
> 函数使用warp内置的`__shfl_down_sync`指令实现高效的warp内归约，找到最大值后，使用`__shfl_sync`指令将结果广播到warp内所有线程
>
> Softmax计算被分为三个步骤，每个步骤都使用warp内协作处理：
>
> 1. 找最大值 + warp归约
> 2. 计算指数和累加 + warp归约
> 3. 归一化
>
> 这减少了全局内存的访问次数，每个元素只需从全局内存读一次、写两次，而不是原来的读三次、写两次。
>
> 这个优化版本的Softmax计算相比原始版本有以下性能优势：
>
> 1. **内存访问效率**：
>    - 减少了全局内存的访问次数
>    - 更好的内存访问模式（warp内线程处理连续内存区域）
> 2. **计算效率**：
>    - 利用了warp内线程并行处理
>    - 使用了高效的warp级别归约和广播操作
> 3. **硬件利用率**：
>    - 更好地利用了GPU的SIMT（单指令多线程）架构
>    - 减少了warp分歧



threads_per_block 32  ：2031.003430 

threads_per_block 64  ：2023.681137

threads_per_block 128：2032.730586

threads_per_block 256：2019.009669 

```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

const char* version_name = "Warp-optimized Softmax implementation.";

#define TILE_SIZE 16
#define WARP_SIZE 32

__global__ void matmul_QKT_shared(int n, float* Q, float* K, float* QKT, float scale) {
    __shared__ float tile_Q[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_K[TILE_SIZE][TILE_SIZE];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    for (int tile = 0; tile < (n + TILE_SIZE - 1) / TILE_SIZE; ++tile) {
        if (row < n && tile * TILE_SIZE + tx < n)
            tile_Q[ty][tx] = Q[row * n + tile * TILE_SIZE + tx];
        else
            tile_Q[ty][tx] = 0.0f;
        
        if (col < n && tile * TILE_SIZE + ty < n)
            tile_K[ty][tx] = K[col * n + tile * TILE_SIZE + ty]; 
        else
            tile_K[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += tile_Q[ty][k] * tile_K[k][tx];
        }

        __syncthreads();
    }
    
    if (row < n && col < n)
        QKT[row * n + col] = sum * scale;
}

__global__ void softmax_warp_optimized(int n, float* QKT) {
    int row = blockIdx.x * (blockDim.x / WARP_SIZE) + threadIdx.x / WARP_SIZE;
    int lane_id = threadIdx.x % WARP_SIZE;
    
    if (row < n) {
        float max_val = -INFINITY;
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            float val = QKT[row * n + j];
            max_val = fmaxf(max_val, val);
        }
        
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
            max_val = fmaxf(max_val, __shfl_down_sync(0xffffffff, max_val, offset));
        }
        
        max_val = __shfl_sync(0xffffffff, max_val, 0);
        
        float sum_exp = 0.0f;
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            float val = expf(QKT[row * n + j] - max_val);
            QKT[row * n + j] = val; // 存储中间结果
            sum_exp += val;
        }
        
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
            sum_exp += __shfl_down_sync(0xffffffff, sum_exp, offset);
        }
        
        sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);
        
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            QKT[row * n + j] /= sum_exp;
        }
    }
}

__global__ void matmul_softmax_V_shared(int n, float* softmax_QKT, float* V, float* Y) {
    __shared__ float tile_S[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_V[TILE_SIZE][TILE_SIZE];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    for (int tile = 0; tile < (n + TILE_SIZE - 1) / TILE_SIZE; ++tile) {
        if (row < n && tile * TILE_SIZE + tx < n)
            tile_S[ty][tx] = softmax_QKT[row * n + tile * TILE_SIZE + tx];
        else
            tile_S[ty][tx] = 0.0f;
            
        if (tile * TILE_SIZE + ty < n && col < n)
            tile_V[ty][tx] = V[(tile * TILE_SIZE + ty) * n + col];
        else
            tile_V[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += tile_S[ty][k] * tile_V[k][tx];
        }
        
        __syncthreads();
    }
    
    if (row < n && col < n)
        Y[row * n + col] = sum;
}

void square_attention(int n, float* gpu_Q, float* gpu_K, float* gpu_V, float* gpu_Y) {
    float* gpu_QKT;
    cudaMalloc(&gpu_QKT, n * n * sizeof(float));
    
    float scale = 1.0f / sqrtf(n);
    
    dim3 block_dim(TILE_SIZE, TILE_SIZE);
    dim3 grid_dim((n + TILE_SIZE - 1) / TILE_SIZE, 
                  (n + TILE_SIZE - 1) / TILE_SIZE);
    
    int threads_per_block = 128;
    int warps_per_block = threads_per_block / WARP_SIZE;
    int num_blocks = (n + warps_per_block - 1) / warps_per_block;
    
    matmul_QKT_shared<<<grid_dim, block_dim>>>(n, gpu_Q, gpu_K, gpu_QKT, scale);

    softmax_warp_optimized<<<num_blocks, threads_per_block>>>(n, gpu_QKT);
    
    matmul_softmax_V_shared<<<grid_dim, block_dim>>>(n, gpu_QKT, gpu_V, gpu_Y);
    
    cudaFree(gpu_QKT);
    
    cudaDeviceSynchronize();
}
```

