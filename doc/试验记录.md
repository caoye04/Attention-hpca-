# result

| Size     | Version1_naive_GPU | Version2\_编译优化_GPU | Version3\_共享内存优化_GPU | Version4\_Warp级别Softmax计算优化_GPU | Version5\_寄存器分块&转置数据存储&循环展开的矩阵优化实现_GPU | Version6\_向量化_GPU |
| -------- | ------------------ | ---------------------- | -------------------------- | ------------------------------------- | ------------------------------------------------------------ | -------------------- |
| 63       | 7.63               | 8.09                   | 7.89                       | 8.16                                  | 8.25                                                         | 8.33                 |
| 64       | 7.82               | 8.13                   | 8.09                       | 8.39                                  | 8.70                                                         | 8.76                 |
| 65       | 8.42               | 8.93                   | 8.48                       | 8.95                                  | 8.28                                                         | 8.47                 |
| 127      | 49.4               | 51.6                   | 49.8                       | 52.3                                  | 58.4                                                         | 61.3                 |
| 128      | 45.4               | 46.2                   | 46.0                       | 48.4                                  | 60.4                                                         | 63.1                 |
| 129      | 51.8               | 53.6                   | 50.5                       | 54.0                                  | 56.6                                                         | 60.1                 |
| 191      | 119                | 120                    | 120                        | 124                                   | 175                                                          | 190                  |
| 192      | 88.6               | 93.8                   | 98.8                       | 102                                   | 179                                                          | 194                  |
| 193      | 120                | 120                    | 121                        | 124                                   | 168                                                          | 184                  |
| 255      | 196                | 200                    | 214                        | 211                                   | 374                                                          | 416                  |
| 256      | 146                | 148                    | 177                        | 176                                   | 382                                                          | 424                  |
| 257      | 199                | 202                    | 214                        | 213                                   | 361                                                          | 404                  |
| 319      | 284                | 283                    | 303                        | 321                                   | 659                                                          | 752                  |
| 320      | 191                | 198                    | 230                        | 241                                   | 668                                                          | 762                  |
| 321      | 285                | 284                    | 303                        | 318                                   | 635                                                          | 725                  |
| 383      | 409                | 410                    | 472                        | 479                                   | 1040                                                         | 1200                 |
| 384      | 272                | 279                    | 361                        | 366                                   | 1060                                                         | 1230                 |
| 385      | 398                | 399                    | 433                        | 469                                   | 1000                                                         | 1170                 |
| 447      | 545                | 553                    | 621                        | 646                                   | 1520                                                         | 1800                 |
| 448      | 351                | 357                    | 499                        | 521                                   | 1540                                                         | 1810                 |
| 449      | 546                | 552                    | 615                        | 633                                   | 1470                                                         | 1740                 |
| 511      | 678                | 691                    | 768                        | 813                                   | 2070                                                         | 2470                 |
| 512      | 418                | 423                    | 630                        | 667                                   | 2100                                                         | 2500                 |
| 513      | 682                | 694                    | 755                        | 809                                   | 2030                                                         | 2430                 |
| 575      | 742                | 747                    | 947                        | 955                                   | 2280                                                         | 2930                 |
| 576      | 450                | 458                    | 745                        | 752                                   | 2280                                                         | 2940                 |
| 577      | 741                | 745                    | 870                        | 938                                   | 2240                                                         | 2770                 |
| 639      | 736                | 734                    | 1040                       | 1060                                  | 2720                                                         | 3530                 |
| 640      | 468                | 476                    | 805                        | 817                                   | 2730                                                         | 3560                 |
| 641      | 735                | 733                    | 966                        | 1040                                  | 2820                                                         | 3540                 |
| 703      | 943                | 953                    | 1240                       | 1260                                  | 3380                                                         | 4460                 |
| 704      | 547                | 552                    | 1020                       | 1040                                  | 3400                                                         | 4450                 |
| 705      | 936                | 946                    | 1140                       | 1230                                  | 3500                                                         | 4470                 |
| 767      | 904                | 905                    | 1300                       | 1320                                  | 3620                                                         | 4640                 |
| 768      | 555                | 562                    | 1040                       | 1070                                  | 3650                                                         | 4700                 |
| 769      | 898                | 898                    | 1210                       | 1300                                  | 3220                                                         | 3710                 |
| 831      | 1040               | 1050                   | 1390                       | 1470                                  | 2640                                                         | 3590                 |
| 832      | 596                | 602                    | 1140                       | 1210                                  | 2660                                                         | 3620                 |
| 833      | 1030               | 1040                   | 1360                       | 1440                                  | 2800                                                         | 3660                 |
| 895      | 1130               | 1140                   | 1490                       | 1600                                  | 2880                                                         | 4010                 |
| 896      | 633                | 637                    | 1270                       | 1350                                  | 2880                                                         | 4020                 |
| 897      | 1120               | 1130                   | 1450                       | 1570                                  | 3270                                                         | 4280                 |
| 959      | 1020               | 1010                   | 1570                       | 1660                                  | 3310                                                         | 4640                 |
| 960      | 627                | 634                    | 1290                       | 1350                                  | 3310                                                         | 4650                 |
| 961      | 1010               | 1000                   | 1510                       | 1620                                  | 3730                                                         | 4910                 |
| 1023     | 1310               | 1320                   | 1730                       | 1820                                  | 3790                                                         | 5320                 |
| 1024     | 686                | 689                    | 1480                       | 1570                                  | 3810                                                         | 5330                 |
| 1025     | 1270               | 1280                   | 1650                       | 1730                                  | 4100                                                         | 5340                 |
| 1087     | 1120               | 1120                   | 1760                       | 1820                                  | 4080                                                         | 5660                 |
| 1088     | 665                | 669                    | 1470                       | 1530                                  | 4090                                                         | 5670                 |
| 1089     | 1120               | 1120                   | 1680                       | 1800                                  | 4610                                                         | 5860                 |
| 1151     | 1240               | 1250                   | 1810                       | 1940                                  | 4570                                                         | 6390                 |
| 1152     | 695                | 700                    | 1550                       | 1650                                  | 4580                                                         | 6400                 |
| 1153     | 1230               | 1230                   | 1730                       | 1900                                  | 3840                                                         | 5150                 |
| 1215     | 1290               | 1300                   | 1830                       | 2030                                  | 3520                                                         | 5070                 |
| 1216     | 712                | 715                    | 1600                       | 1760                                  | 3530                                                         | 5060                 |
| 1217     | 1280               | 1290                   | 1750                       | 1980                                  | 4240                                                         | 5570                 |
| 1279     | 1040               | 1080                   | 1790                       | 2000                                  | 3810                                                         | 5430                 |
| 1280     | 687                | 692                    | 1540                       | 1710                                  | 3820                                                         | 5430                 |
| 1281     | 1040               | 1080                   | 1730                       | 1960                                  | 4560                                                         | 6070                 |
| 1343     | 1180               | 1250                   | 1940                       | 2110                                  | 4220                                                         | 6020                 |
| 1344     | 713                | 718                    | 1670                       | 1830                                  | 4240                                                         | 6020                 |
| 1345     | 1170               | 1240                   | 1810                       | 2060                                  | 5000                                                         | 6570                 |
| 1407     | 1270               | 1360                   | 1950                       | 2180                                  | 4650                                                         | 6620                 |
| 1408     | 733                | 736                    | 1720                       | 1930                                  | 4660                                                         | 6620                 |
| 1409     | 1250               | 1350                   | 1880                       | 2140                                  | 4380                                                         | 5940                 |
| 1471     | 1240               | 1320                   | 1980                       | 2210                                  | 3850                                                         | 5510                 |
| 1472     | 739                | 741                    | 1750                       | 1970                                  | 3850                                                         | 5510                 |
| 1473     | 1230               | 1310                   | 1910                       | 2150                                  | 4640                                                         | 6180                 |
| 1535     | 1150               | 1220                   | 2010                       | 2210                                  | 4200                                                         | 6030                 |
| 1536     | 733                | 737                    | 1770                       | 1960                                  | 4210                                                         | 6040                 |
| 1537     | 1140               | 1210                   | 1950                       | 2170                                  | 5090                                                         | 6860                 |
| 1599     | 1180               | 1250                   | 2060                       | 2240                                  | 4560                                                         | 6540                 |
| 1600     | 732                | 737                    | 1800                       | 1980                                  | 4570                                                         | 6540                 |
| 1601     | 1170               | 1240                   | 2000                       | 2200                                  | 4760                                                         | 6400                 |
| 1663     | 1290               | 1350                   | 2080                       | 2280                                  | 3990                                                         | 5750                 |
| 1664     | 754                | 756                    | 1840                       | 2070                                  | 3980                                                         | 5730                 |
| 1665     | 1270               | 1340                   | 2020                       | 2230                                  | 4860                                                         | 6580                 |
| 1727     | 1260               | 1320                   | 2130                       | 2300                                  | 4280                                                         | 6150                 |
| 1728     | 756                | 755                    | 1880                       | 2100                                  | 4280                                                         | 6150                 |
| 1729     | 1250               | 1310                   | 2030                       | 2260                                  | 5200                                                         | 7000                 |
| 1791     | 1340               | 1400                   | 2150                       | 2300                                  | 4550                                                         | 6500                 |
| 1792     | 763                | 762                    | 1920                       | 2150                                  | 4550                                                         | 6490                 |
| 1793     | 1330               | 1380                   | 2100                       | 2280                                  | 5170                                                         | 6880                 |
| 1855     | 1300               | 1340                   | 2160                       | 2330                                  | 4180                                                         | 6010                 |
| 1856     | 766                | 765                    | 1930                       | 2170                                  | 4170                                                         | 6000                 |
| 1857     | 1280               | 1340                   | 2070                       | 2300                                  | 5090                                                         | 6850                 |
| 1919     | 1160               | 1200                   | 2160                       | 2330                                  | 4420                                                         | 6310                 |
| 1920     | 751                | 754                    | 1930                       | 2140                                  | 4400                                                         | 6290                 |
| 1921     | 1150               | 1190                   | 2070                       | 2290                                  | 5410                                                         | 7220                 |
| 1983     | 1340               | 1390                   | 2230                       | 2370                                  | 4740                                                         | 6760                 |
| 1984     | 774                | 775                    | 2000                       | 2210                                  | 4730                                                         | 6750                 |
| 1985     | 1330               | 1380                   | 2140                       | 2340                                  | 5110                                                         | 6880                 |
| 2047     | 1470               | 1520                   | 2270                       | 2400                                  | 4440                                                         | 6390                 |
| 2048     | 785                | 784                    | 2040                       | 2240                                  | 4480                                                         | 6410                 |
| 2049     | 1440               | 1490                   | 2170                       | 2350                                  | 5390                                                         | 7220                 |
| 4095     | 1560               | 1580                   | 2490                       | 2550                                  | 4930                                                         | 7280                 |
| 4096     | 803                | 800                    | 2270                       | 2460                                  | 5020                                                         | 7310                 |
| 4097     | 1540               | 1550                   | 2440                       | 2530                                  | 5950                                                         | 7350                 |
| 8191     | 1590               | 1590                   | 2550                       | 2500                                  | 4890                                                         | 7600                 |
| 8192     | 818                | 817                    | 2340                       | 2500                                  | 5000                                                         | 7520                 |
| 8193     | 1560               | 1570                   | 2520                       | 2500                                  | 4970                                                         | 7050                 |
| 平均性能 | 837.263590         | 855.904458             | 1514.408617                | 2032.730586                           | 3303.426590                                                  | 4522.412900          |

# 并行计算基础HW—Attention

## 1. 作业背景

Attention 机制最初用于提升序列到序列模型（Seq2Seq）在处理长文本时的效果，通过允许模型动态聚焦输入序列的不同部分，解决了传统方法的瓶颈。它通过计算查询（Q）、键（K）和值（V）之间的相似度来加权每个值，从而生成新的表示。Attention 被广泛应用于 NLP、计算机视觉等领域，尤其是 Transformer 模型中。随着模型应用的扩展，优化 Attention 计算的效率变得尤为重要，常见的优化技巧包括稀疏化计算、低秩近似、多机分布式计算以及 GPU 加速等方法，以减少计算和内存开销，提升效率。

## 2. 作业任务：方阵的 Attention 机制计算

### 2.1. 任务描述

完成 Attention 算子在GPU 单卡上的实现与优化。Attention 算子：

$$
Y=\operatorname{Softmax}\left(\frac{Q K^{\mathrm{T}}}{\sqrt{N}}\right) V
$$


其中，$Q, K, V$ 均为 $N \times N$ 的单精度行优先存储的稠密矩阵。
其中 Softmax 的定义为

$$
\operatorname{Softmax}\left(\left[z_1, \ldots, z_n\right]\right)=\left[\frac{e^{z_1}}{\sum_{j=1}^n e^{z_j}}, \ldots, \frac{e^{z_n}}{\sum_{j=1}^n e^{z_j}}\right]
$$


在 Attention 计算中，Softmax 的计算是逐行进行的，具体算法实现可以参考＂code／cpu／naive．c＂中的代码。在熟悉代码核心后，优化版本的程序实现在＂code／gpu／opt．cu＂中。

### 2.2 正确性验证

1．采用 float 单精度浮点数据类型进行运算，运算结果通过作业基础代码中的正确性验证，eps 为机器的单精度极小值，约为 $10^{-6}$ 左右。

$$
\| \text { custom-attention }(n, Q, K, V)-\text { Label } \|<100 n^2 \epsilon
$$

其中，$\|\cdot\|$ 表示将矩阵逐元素取绝对值求和（即向量的 1 范数）。

2．Attention 的主要计算开销在两次矩阵乘法，其计算复杂度为 $O\left(N^3\right)$ ，在计算性能指标的时候采用 $\left(4 N^3\right)$ 计算，如果采用了一些非 $O\left(N^3\right)$ 算法而导致通过不了正确性测试，这种情况可以适当且合理地放宽精度的要求，但需要在作业报告中指出。

3．开展必要的性能分析，比如某些矩阵规模性能出现明显的降低，可以采用性能分析工具进行性能分析。

## 3. 作业评分

1. 通过正确性检验（ $20 \%$ ）。

2. 评测 Attention 算子在不同输入（共 102 个测例）下的性能结果，按照提交后的性能排序结果，以及代码质量进行打分（ $50 \%$ ）。

3. 详细描述在实现 Attention 算子中采取的优化手段，代码对应的部分，以及对应的实验结果，可以采用性能工具或者模型来解释目前取得的性能结果（ $20 \%$ ）。

4. 给出一张完整的实验结果图，描述当前算法的性能，横坐标为矩阵规模，纵坐标为 Gflop $/ s(10 \%)$ 。

5. 可以参考矩阵乘法相关的外部库（如 BLAS 或 BLIS 等数学库）的实现与优化思路，但禁止直接使用外部库作为作业成果。

> 作业提示：
>
> 1．在保证正确性的前提下，可对计算流程中的某些冗余部分进行删减。
>
> 2．本作业的核心模块是两次矩阵乘法，可以借鉴第一次作业 SGEMM的优化思路，在 GPU 上优化好矩阵运算的子模块。
>
> 3．有任何问题欢迎与助教和老师交流。





# 我的实验：

### Version1：naive

![version1_report_NS](pic\version1_report_NS.png)



### Version2 ：编译优化

naive：837.263590

-O3：837.499209 

-use_fast_math：855.904458 

-arch=sm_86：837.628005 

```cmd
(base) [t2021012167@admin gpu]$ srun --gres=gpu:1 ./check_gpu
GPU Name: NVIDIA A40
Compute Capability: 8.6
```

![version2_report_NS](pic\version2_report_NS.png)

### Version3 ：内存访问优化

naive版本中，实现矩阵乘法直接从全局内存读取数据，这会导致较高的内存延迟。尝试使用共享内存来缓存数据块

TILE_SIZE 8   ：1221.453162 

TILE_SIZE 16 ：1514.408617 

TILE_SIZE 32 ：1402.859883 

![version3_report_NS](pic\version3_report_NS.png)

### Version4：Warp级别Softmax优化实现

> 在原始实现中，每个线程负责处理一整行的Softmax计算，这导致：
>
> - 每个线程需要多次遍历矩阵行
> - 线程之间完全独立，没有利用warp内协作
>
> 新版本中：
>
> - 每个warp（32个线程）协作处理一行
> - 行内的元素被warp内的线程平均分配
> - 通过线程协作，减少了全局内存的遍历次数
>
> 函数使用warp内置的`__shfl_down_sync`指令实现高效的warp内归约，找到最大值后，使用`__shfl_sync`指令将结果广播到warp内所有线程
>
> Softmax计算被分为三个步骤，每个步骤都使用warp内协作处理：
>
> 1. 找最大值 + warp归约
> 2. 计算指数和累加 + warp归约
> 3. 归一化
>
> 这减少了全局内存的访问次数，每个元素只需从全局内存读一次、写两次，而不是原来的读三次、写两次。
>
> 这个优化版本的Softmax计算相比原始版本有以下性能优势：
>
> 1. **内存访问效率**：
>    - 减少了全局内存的访问次数
>    - 更好的内存访问模式（warp内线程处理连续内存区域）
> 2. **计算效率**：
>    - 利用了warp内线程并行处理
>    - 使用了高效的warp级别归约和广播操作
> 3. **硬件利用率**：
>    - 更好地利用了GPU的SIMT（单指令多线程）架构
>    - 减少了warp分歧



threads_per_block 32  ：2031.003430 

threads_per_block 64  ：2023.681137

threads_per_block 128：2032.730586

threads_per_block 256：2019.009669 



![version4_report_NS](pic\version4_report_NS.png)

### Version5：寄存器分块&转置数据存储&循环展开的矩阵优化实现

1. 寄存器分块: 每个线程计算输出矩阵的4×4块，这样做:
   - 提高了每个线程的算术密度
   - 改善了寄存器(最快的内存)中的数据重用
   - 降低了内存操作与计算操作的比率
2. 优化的内存访问模式:
   - K和V矩阵在共享内存中采用转置数据存储
   - 在共享内存数组中添加填充以避免bank冲突
   - 协作式加载数据到共享内存以最大化合并访问
3. 循环展开和寄存器缓存:
   - 使用`#pragma unroll`进行显式循环展开
   - 在寄存器中缓存输入值以避免重复的共享内存查找
   - 在寄存器中存储中间结果以最小化共享内存流量

主要测试了一下这里的REG_TILE_SIZE参数大小

最终发现最优是：4×4的寄存器

![version5_report_NS](pic\version5_report_NS.png)

### Version6：向量化实现

1. 向量化加载优化:
   - 使用2元素分组进行内存加载，减少内存访问事务数量
   - 每个线程一次加载多个相邻元素，提高内存带宽利用率
   - 比原始的单元素加载提高近2倍的加载效率
2. 保守向量化策略:
   - 采用每次处理2个元素的保守向量化方式
   - 为每个加载元素提供明确的边界检查，确保计算精度
   - 平衡了性能提升与数值稳定性
3. 共享内存布局优化:
   - 维持了填充的共享内存布局以避免bank冲突
   - 转置数据存储结合向量化加载，优化后续计算的访问模式
   - 精心设计的共享内存布局提高了缓存命中率
4. 内核执行优化:
   - 为计算密集型循环提高了循环展开度(从4到8)
   - 保持高效的寄存器使用模式，避免寄存器溢出
   - 协作式线程模式确保最大化计算资源利用率

向量化实现在保持正确性的同时，进一步提高了之前版本的性能，特别是在处理较大矩阵时。通过系统地结合寄存器分块、共享内存优化和向量化加载技术，我们为Attention算子实现了高效的GPU计算。

![version6_report_NS](pic\version6_report_NS.png)









---

# 附录：每个版本的代码

## Version1

```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

const char* version_name = "Optimized implementation.";


__global__ void matmul_QKT(int n, float* Q, float* K, float* QKT, float scale) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < n && col < n) {
        float sum = 0.0f;
        for (int k = 0; k < n; k++) {
            sum += Q[row * n + k] * K[col * n + k]; 
        }
        QKT[row * n + col] = sum * scale;
    }
}

__global__ void softmax_kernel(int n, float* QKT) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < n) {
        float max_val = -INFINITY;
        for (int j = 0; j < n; j++) {
            if (QKT[row * n + j] > max_val) {
                max_val = QKT[row * n + j];
            }
        }
        float sum_exp = 0.0f;
        for (int j = 0; j < n; j++) {
            QKT[row * n + j] = expf(QKT[row * n + j] - max_val);
            sum_exp += QKT[row * n + j];
        }
        
        for (int j = 0; j < n; j++) {
            QKT[row * n + j] /= sum_exp;
        }
    }
}

__global__ void matmul_softmax_V(int n, float* softmax_QKT, float* V, float* Y) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < n && col < n) {
        float sum = 0.0f;
        for (int k = 0; k < n; k++) {
            sum += softmax_QKT[row * n + k] * V[k * n + col];
        }
        Y[row * n + col] = sum;
    }
}

void square_attention(int n, float* gpu_Q, float* gpu_K, float* gpu_V, float* gpu_Y) {

    float* gpu_QKT;
    cudaMalloc(&gpu_QKT, n * n * sizeof(float));
    
    float scale = 1.0f / sqrtf(n);
    
    dim3 block_dim(16, 16);
    dim3 grid_dim((n + block_dim.x - 1) / block_dim.x, 
                  (n + block_dim.y - 1) / block_dim.y);

    dim3 softmax_block_dim(256);
    dim3 softmax_grid_dim((n + softmax_block_dim.x - 1) / softmax_block_dim.x);
    
    matmul_QKT<<<grid_dim, block_dim>>>(n, gpu_Q, gpu_K, gpu_QKT, scale);

    softmax_kernel<<<softmax_grid_dim, softmax_block_dim>>>(n, gpu_QKT);
    
    matmul_softmax_V<<<grid_dim, block_dim>>>(n, gpu_QKT, gpu_V, gpu_Y);
    
    cudaFree(gpu_QKT);
    
    cudaDeviceSynchronize();
}
```

## Version2

```
CUDAOPT = -use_fast_math
```

## Version3

```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

const char* version_name = "Shared Memory Optimized implementation.";

#define TILE_SIZE 16

__global__ void matmul_QKT_shared(int n, float* Q, float* K, float* QKT, float scale) {
    __shared__ float tile_Q[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_K[TILE_SIZE][TILE_SIZE];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    for (int tile = 0; tile < (n + TILE_SIZE - 1) / TILE_SIZE; ++tile) {
        if (row < n && tile * TILE_SIZE + tx < n)
            tile_Q[ty][tx] = Q[row * n + tile * TILE_SIZE + tx];
        else
            tile_Q[ty][tx] = 0.0f;
        
        if (col < n && tile * TILE_SIZE + ty < n)
            tile_K[ty][tx] = K[col * n + tile * TILE_SIZE + ty]; 
        else
            tile_K[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += tile_Q[ty][k] * tile_K[k][tx];
        }

        __syncthreads();
    }
    
    if (row < n && col < n)
        QKT[row * n + col] = sum * scale;
}

__global__ void softmax_kernel(int n, float* QKT) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (row < n) {
        float max_val = -INFINITY;
        for (int j = 0; j < n; j++) {
            if (QKT[row * n + j] > max_val) {
                max_val = QKT[row * n + j];
            }
        }
        float sum_exp = 0.0f;
        for (int j = 0; j < n; j++) {
            QKT[row * n + j] = expf(QKT[row * n + j] - max_val);
            sum_exp += QKT[row * n + j];
        }
        
        for (int j = 0; j < n; j++) {
            QKT[row * n + j] /= sum_exp;
        }
    }
}

__global__ void matmul_softmax_V_shared(int n, float* softmax_QKT, float* V, float* Y) {
    __shared__ float tile_S[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_V[TILE_SIZE][TILE_SIZE];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    for (int tile = 0; tile < (n + TILE_SIZE - 1) / TILE_SIZE; ++tile) {
        if (row < n && tile * TILE_SIZE + tx < n)
            tile_S[ty][tx] = softmax_QKT[row * n + tile * TILE_SIZE + tx];
        else
            tile_S[ty][tx] = 0.0f;
            
        if (tile * TILE_SIZE + ty < n && col < n)
            tile_V[ty][tx] = V[(tile * TILE_SIZE + ty) * n + col];
        else
            tile_V[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += tile_S[ty][k] * tile_V[k][tx];
        }
        
        __syncthreads();
    }
    
    if (row < n && col < n)
        Y[row * n + col] = sum;
}

void square_attention(int n, float* gpu_Q, float* gpu_K, float* gpu_V, float* gpu_Y) {
    float* gpu_QKT;
    cudaMalloc(&gpu_QKT, n * n * sizeof(float));
    
    float scale = 1.0f / sqrtf(n);
    
    dim3 block_dim(TILE_SIZE, TILE_SIZE);
    dim3 grid_dim((n + TILE_SIZE - 1) / TILE_SIZE, 
                  (n + TILE_SIZE - 1) / TILE_SIZE);
    
    dim3 softmax_block_dim(256);
    dim3 softmax_grid_dim((n + softmax_block_dim.x - 1) / softmax_block_dim.x);
    
    matmul_QKT_shared<<<grid_dim, block_dim>>>(n, gpu_Q, gpu_K, gpu_QKT, scale);
    
    softmax_kernel<<<softmax_grid_dim, softmax_block_dim>>>(n, gpu_QKT);
    
    matmul_softmax_V_shared<<<grid_dim, block_dim>>>(n, gpu_QKT, gpu_V, gpu_Y);
    
    cudaFree(gpu_QKT);
    
    cudaDeviceSynchronize();
}
```

## Version4

```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

const char* version_name = "Warp-optimized Softmax implementation.";

#define TILE_SIZE 16
#define WARP_SIZE 32

__global__ void matmul_QKT_shared(int n, float* Q, float* K, float* QKT, float scale) {
    __shared__ float tile_Q[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_K[TILE_SIZE][TILE_SIZE];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    for (int tile = 0; tile < (n + TILE_SIZE - 1) / TILE_SIZE; ++tile) {
        if (row < n && tile * TILE_SIZE + tx < n)
            tile_Q[ty][tx] = Q[row * n + tile * TILE_SIZE + tx];
        else
            tile_Q[ty][tx] = 0.0f;
        
        if (col < n && tile * TILE_SIZE + ty < n)
            tile_K[ty][tx] = K[col * n + tile * TILE_SIZE + ty]; 
        else
            tile_K[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += tile_Q[ty][k] * tile_K[k][tx];
        }

        __syncthreads();
    }
    
    if (row < n && col < n)
        QKT[row * n + col] = sum * scale;
}

__global__ void softmax_warp_optimized(int n, float* QKT) {
    int row = blockIdx.x * (blockDim.x / WARP_SIZE) + threadIdx.x / WARP_SIZE;
    int lane_id = threadIdx.x % WARP_SIZE;
    
    if (row < n) {
        float max_val = -INFINITY;
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            float val = QKT[row * n + j];
            max_val = fmaxf(max_val, val);
        }
        
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
            max_val = fmaxf(max_val, __shfl_down_sync(0xffffffff, max_val, offset));
        }
        
        max_val = __shfl_sync(0xffffffff, max_val, 0);
        
        float sum_exp = 0.0f;
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            float val = expf(QKT[row * n + j] - max_val);
            QKT[row * n + j] = val;
            sum_exp += val;
        }
        
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
            sum_exp += __shfl_down_sync(0xffffffff, sum_exp, offset);
        }
        
        sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);
        
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            QKT[row * n + j] /= sum_exp;
        }
    }
}

__global__ void matmul_softmax_V_shared(int n, float* softmax_QKT, float* V, float* Y) {
    __shared__ float tile_S[TILE_SIZE][TILE_SIZE];
    __shared__ float tile_V[TILE_SIZE][TILE_SIZE];
    
    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty;
    int col = bx * TILE_SIZE + tx;
    
    float sum = 0.0f;
    
    for (int tile = 0; tile < (n + TILE_SIZE - 1) / TILE_SIZE; ++tile) {
        if (row < n && tile * TILE_SIZE + tx < n)
            tile_S[ty][tx] = softmax_QKT[row * n + tile * TILE_SIZE + tx];
        else
            tile_S[ty][tx] = 0.0f;
            
        if (tile * TILE_SIZE + ty < n && col < n)
            tile_V[ty][tx] = V[(tile * TILE_SIZE + ty) * n + col];
        else
            tile_V[ty][tx] = 0.0f;
            
        __syncthreads();
        
        for (int k = 0; k < TILE_SIZE; ++k) {
            sum += tile_S[ty][k] * tile_V[k][tx];
        }
        
        __syncthreads();
    }
    
    if (row < n && col < n)
        Y[row * n + col] = sum;
}

void square_attention(int n, float* gpu_Q, float* gpu_K, float* gpu_V, float* gpu_Y) {
    float* gpu_QKT;
    cudaMalloc(&gpu_QKT, n * n * sizeof(float));
    
    float scale = 1.0f / sqrtf(n);
    
    dim3 block_dim(TILE_SIZE, TILE_SIZE);
    dim3 grid_dim((n + TILE_SIZE - 1) / TILE_SIZE, 
                  (n + TILE_SIZE - 1) / TILE_SIZE);
    
    int threads_per_block = 128;
    int warps_per_block = threads_per_block / WARP_SIZE;
    int num_blocks = (n + warps_per_block - 1) / warps_per_block;
    
    matmul_QKT_shared<<<grid_dim, block_dim>>>(n, gpu_Q, gpu_K, gpu_QKT, scale);

    softmax_warp_optimized<<<num_blocks, threads_per_block>>>(n, gpu_QKT);
    
    matmul_softmax_V_shared<<<grid_dim, block_dim>>>(n, gpu_QKT, gpu_V, gpu_Y);
    
    cudaFree(gpu_QKT);
    
    cudaDeviceSynchronize();
}
```

## Version5

```cuda
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

const char* version_name = "Opt model";

#define TILE_SIZE 64
#define REG_TILE_SIZE 4
#define BLOCK_DIM (TILE_SIZE / REG_TILE_SIZE)
#define WARP_SIZE 32

__global__ void matmul_QKT_register_blocked(int n, float* Q, float* K, float* QKT, float scale) {
    __shared__ float s_Q[TILE_SIZE][TILE_SIZE+1];
    __shared__ float s_K[TILE_SIZE][TILE_SIZE+1]; 
    
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty * REG_TILE_SIZE;
    int col = bx * TILE_SIZE + tx * REG_TILE_SIZE;
    
    float sum[REG_TILE_SIZE][REG_TILE_SIZE] = {0.0f};
    
    for (int t = 0; t < (n + TILE_SIZE - 1) / TILE_SIZE; ++t) {
        for (int i = ty; i < TILE_SIZE; i += blockDim.y) {
            for (int j = tx; j < TILE_SIZE; j += blockDim.x) {
                int r = by * TILE_SIZE + i;
                int c = t * TILE_SIZE + j;
                if (r < n && c < n) {
                    s_Q[i][j] = Q[r * n + c];
                } else {
                    s_Q[i][j] = 0.0f;
                }
            }
        }
        
        for (int i = ty; i < TILE_SIZE; i += blockDim.y) {
            for (int j = tx; j < TILE_SIZE; j += blockDim.x) {
                int r = t * TILE_SIZE + i;
                int c = bx * TILE_SIZE + j;
                if (r < n && c < n) {
                    s_K[j][i] = K[c * n + r];
                } else {
                    s_K[j][i] = 0.0f;
                }
            }
        }
        
        __syncthreads();
        
        #pragma unroll 4
        for (int k = 0; k < TILE_SIZE; ++k) {
            float q_vals[REG_TILE_SIZE];
            #pragma unroll
            for (int i = 0; i < REG_TILE_SIZE; ++i) {
                q_vals[i] = s_Q[ty * REG_TILE_SIZE + i][k];
            }
            
            float k_vals[REG_TILE_SIZE];
            #pragma unroll
            for (int j = 0; j < REG_TILE_SIZE; ++j) {
                k_vals[j] = s_K[tx * REG_TILE_SIZE + j][k];
            }
            
            #pragma unroll
            for (int i = 0; i < REG_TILE_SIZE; ++i) {
                #pragma unroll
                for (int j = 0; j < REG_TILE_SIZE; ++j) {
                    sum[i][j] += q_vals[i] * k_vals[j];
                }
            }
        }
        
        __syncthreads();
    }
    
    for (int i = 0; i < REG_TILE_SIZE; ++i) {
        int r = row + i;
        if (r < n) {
            for (int j = 0; j < REG_TILE_SIZE; ++j) {
                int c = col + j;
                if (c < n) {
                    QKT[r * n + c] = sum[i][j] * scale;
                }
            }
        }
    }
}

__global__ void softmax_warp_optimized(int n, float* QKT) {
    int row = blockIdx.x * (blockDim.x / WARP_SIZE) + threadIdx.x / WARP_SIZE;
    int lane_id = threadIdx.x % WARP_SIZE;
    
    if (row < n) {
        float max_val = -INFINITY;
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            float val = QKT[row * n + j];
            max_val = fmaxf(max_val, val);
        }
        
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
            max_val = fmaxf(max_val, __shfl_down_sync(0xffffffff, max_val, offset));
        }
        
        max_val = __shfl_sync(0xffffffff, max_val, 0);
        
        float sum_exp = 0.0f;
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            float val = expf(QKT[row * n + j] - max_val);
            QKT[row * n + j] = val; // Store intermediate result
            sum_exp += val;
        }
        
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
            sum_exp += __shfl_down_sync(0xffffffff, sum_exp, offset);
        }
        
        sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);
        
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            QKT[row * n + j] /= sum_exp;
        }
    }
}

__global__ void matmul_softmax_V_register_blocked(int n, float* softmax_QKT, float* V, float* Y) {
    __shared__ float s_softmax[TILE_SIZE][TILE_SIZE+1]; 
    __shared__ float s_V[TILE_SIZE][TILE_SIZE+1];
    
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty * REG_TILE_SIZE;
    int col = bx * TILE_SIZE + tx * REG_TILE_SIZE;
    
    float sum[REG_TILE_SIZE][REG_TILE_SIZE] = {0.0f};
    
    for (int t = 0; t < (n + TILE_SIZE - 1) / TILE_SIZE; ++t) {
        for (int i = ty; i < TILE_SIZE; i += blockDim.y) {
            for (int j = tx; j < TILE_SIZE; j += blockDim.x) {
                int r = by * TILE_SIZE + i;
                int c = t * TILE_SIZE + j;
                if (r < n && c < n) {
                    s_softmax[i][j] = softmax_QKT[r * n + c];
                } else {
                    s_softmax[i][j] = 0.0f;
                }
            }
        }
        
        for (int i = ty; i < TILE_SIZE; i += blockDim.y) {
            for (int j = tx; j < TILE_SIZE; j += blockDim.x) {
                int r = t * TILE_SIZE + i;
                int c = bx * TILE_SIZE + j;
                if (r < n && c < n) {
                    s_V[j][i] = V[r * n + c];
                } else {
                    s_V[j][i] = 0.0f;
                }
            }
        }
        
        __syncthreads();
        
        #pragma unroll 4
        for (int k = 0; k < TILE_SIZE; ++k) {
            float softmax_vals[REG_TILE_SIZE];
            #pragma unroll
            for (int i = 0; i < REG_TILE_SIZE; ++i) {
                softmax_vals[i] = s_softmax[ty * REG_TILE_SIZE + i][k];
            }
            
            float v_vals[REG_TILE_SIZE];
            #pragma unroll
            for (int j = 0; j < REG_TILE_SIZE; ++j) {
                v_vals[j] = s_V[tx * REG_TILE_SIZE + j][k];
            }
            
            #pragma unroll
            for (int i = 0; i < REG_TILE_SIZE; ++i) {
                #pragma unroll
                for (int j = 0; j < REG_TILE_SIZE; ++j) {
                    sum[i][j] += softmax_vals[i] * v_vals[j];
                }
            }
        }
        
        __syncthreads();
    }
    
    for (int i = 0; i < REG_TILE_SIZE; ++i) {
        int r = row + i;
        if (r < n) {
            for (int j = 0; j < REG_TILE_SIZE; ++j) {
                int c = col + j;
                if (c < n) {
                    Y[r * n + c] = sum[i][j];
                }
            }
        }
    }
}

void square_attention(int n, float* gpu_Q, float* gpu_K, float* gpu_V, float* gpu_Y) {
    float* gpu_QKT;
    cudaMalloc(&gpu_QKT, n * n * sizeof(float));
    
    float scale = 1.0f / sqrtf(n);
    
    dim3 block_dim(BLOCK_DIM, BLOCK_DIM);
    dim3 grid_dim((n + TILE_SIZE - 1) / TILE_SIZE, (n + TILE_SIZE - 1) / TILE_SIZE);
    
    matmul_QKT_register_blocked<<<grid_dim, block_dim>>>(n, gpu_Q, gpu_K, gpu_QKT, scale);
    
    int threads_per_block = 128;
    int warps_per_block = threads_per_block / WARP_SIZE;
    int num_blocks = (n + warps_per_block - 1) / warps_per_block;
    softmax_warp_optimized<<<num_blocks, threads_per_block>>>(n, gpu_QKT);
    
    matmul_softmax_V_register_blocked<<<grid_dim, block_dim>>>(n, gpu_QKT, gpu_V, gpu_Y);
    
    cudaFree(gpu_QKT);
    cudaDeviceSynchronize();
}
```

## Version6

```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <cuda.h>
#include <cuda_runtime.h>
#include <math.h>

const char* version_name = "Conservative vectorized implementation with register blocking";

#define TILE_SIZE 64
#define REG_TILE_SIZE 4
#define BLOCK_DIM (TILE_SIZE / REG_TILE_SIZE)
#define WARP_SIZE 32

__global__ void matmul_QKT_register_blocked(int n, float* Q, float* K, float* QKT, float scale) {
    __shared__ float s_Q[TILE_SIZE][TILE_SIZE+1];
    __shared__ float s_K[TILE_SIZE][TILE_SIZE+1]; 
    
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty * REG_TILE_SIZE;
    int col = bx * TILE_SIZE + tx * REG_TILE_SIZE;
    
    float sum[REG_TILE_SIZE][REG_TILE_SIZE] = {0.0f};
    
    for (int t = 0; t < (n + TILE_SIZE - 1) / TILE_SIZE; ++t) {
        for (int i = ty; i < TILE_SIZE; i += blockDim.y) {
            int r = by * TILE_SIZE + i;
            
            for (int j = tx * 2; j < TILE_SIZE; j += blockDim.x * 2) {
                int c = t * TILE_SIZE + j;
                
                if (r < n && c < n) {
                    s_Q[i][j] = Q[r * n + c];
                } else {
                    s_Q[i][j] = 0.0f;
                }
                
                if (r < n && c + 1 < n) {
                    s_Q[i][j+1] = Q[r * n + c + 1];
                } else {
                    s_Q[i][j+1] = 0.0f;
                }
            }
        }
        
        for (int i = ty; i < TILE_SIZE; i += blockDim.y) {
            int r = t * TILE_SIZE + i;
            
            for (int j = tx * 2; j < TILE_SIZE; j += blockDim.x * 2) {
                int c = bx * TILE_SIZE + j;
                
                if (r < n && c < n) {
                    s_K[j][i] = K[c * n + r];
                } else {
                    s_K[j][i] = 0.0f;
                }
                
                if (r < n && c + 1 < n) {
                    s_K[j+1][i] = K[(c+1) * n + r];
                } else {
                    s_K[j+1][i] = 0.0f;
                }
            }
        }
        
        __syncthreads();
        
        #pragma unroll 4
        for (int k = 0; k < TILE_SIZE; ++k) {
            float q_vals[REG_TILE_SIZE];
            #pragma unroll
            for (int i = 0; i < REG_TILE_SIZE; ++i) {
                q_vals[i] = s_Q[ty * REG_TILE_SIZE + i][k];
            }
            
            float k_vals[REG_TILE_SIZE];
            #pragma unroll
            for (int j = 0; j < REG_TILE_SIZE; ++j) {
                k_vals[j] = s_K[tx * REG_TILE_SIZE + j][k];
            }
            
            #pragma unroll
            for (int i = 0; i < REG_TILE_SIZE; ++i) {
                #pragma unroll
                for (int j = 0; j < REG_TILE_SIZE; ++j) {
                    sum[i][j] += q_vals[i] * k_vals[j];
                }
            }
        }
        
        __syncthreads();
    }
    
    for (int i = 0; i < REG_TILE_SIZE; ++i) {
        int r = row + i;
        if (r < n) {
            for (int j = 0; j < REG_TILE_SIZE; ++j) {
                int c = col + j;
                if (c < n) {
                    QKT[r * n + c] = sum[i][j] * scale;
                }
            }
        }
    }
}

__global__ void softmax_warp_optimized(int n, float* QKT) {
    int row = blockIdx.x * (blockDim.x / WARP_SIZE) + threadIdx.x / WARP_SIZE;
    int lane_id = threadIdx.x % WARP_SIZE;
    
    if (row < n) {
        float max_val = -INFINITY;
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            float val = QKT[row * n + j];
            max_val = fmaxf(max_val, val);
        }
        
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
            max_val = fmaxf(max_val, __shfl_down_sync(0xffffffff, max_val, offset));
        }
        
        max_val = __shfl_sync(0xffffffff, max_val, 0);
        
        float sum_exp = 0.0f;
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            float val = expf(QKT[row * n + j] - max_val);
            QKT[row * n + j] = val;
            sum_exp += val;
        }
        
        #pragma unroll
        for (int offset = WARP_SIZE/2; offset > 0; offset /= 2) {
            sum_exp += __shfl_down_sync(0xffffffff, sum_exp, offset);
        }
        
        sum_exp = __shfl_sync(0xffffffff, sum_exp, 0);
        
        for (int j = lane_id; j < n; j += WARP_SIZE) {
            QKT[row * n + j] /= sum_exp;
        }
    }
}

__global__ void matmul_softmax_V_register_blocked(int n, float* softmax_QKT, float* V, float* Y) {
    __shared__ float s_softmax[TILE_SIZE][TILE_SIZE+1]; 
    __shared__ float s_V[TILE_SIZE][TILE_SIZE+1];
    
    int bx = blockIdx.x;
    int by = blockIdx.y;
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    
    int row = by * TILE_SIZE + ty * REG_TILE_SIZE;
    int col = bx * TILE_SIZE + tx * REG_TILE_SIZE;
    
    float sum[REG_TILE_SIZE][REG_TILE_SIZE] = {0.0f};
    
    for (int t = 0; t < (n + TILE_SIZE - 1) / TILE_SIZE; ++t) {
        for (int i = ty; i < TILE_SIZE; i += blockDim.y) {
            int r = by * TILE_SIZE + i;
            
            for (int j = tx * 2; j < TILE_SIZE; j += blockDim.x * 2) {
                int c = t * TILE_SIZE + j;
                
                if (r < n && c < n) {
                    s_softmax[i][j] = softmax_QKT[r * n + c];
                } else {
                    s_softmax[i][j] = 0.0f;
                }
                
                if (r < n && c + 1 < n) {
                    s_softmax[i][j+1] = softmax_QKT[r * n + c + 1];
                } else {
                    s_softmax[i][j+1] = 0.0f;
                }
            }
        }
        
        for (int i = ty; i < TILE_SIZE; i += blockDim.y) {
            int r = t * TILE_SIZE + i;
            
            for (int j = tx * 2; j < TILE_SIZE; j += blockDim.x * 2) {
                int c = bx * TILE_SIZE + j;
                
                if (r < n && c < n) {
                    s_V[j][i] = V[r * n + c];
                } else {
                    s_V[j][i] = 0.0f;
                }
                
                if (r < n && c + 1 < n) {
                    s_V[j+1][i] = V[r * n + c + 1];
                } else {
                    s_V[j+1][i] = 0.0f;
                }
            }
        }
        
        __syncthreads();
        
        #pragma unroll 8
        for (int k = 0; k < TILE_SIZE; ++k) {
            float softmax_vals[REG_TILE_SIZE];
            #pragma unroll
            for (int i = 0; i < REG_TILE_SIZE; ++i) {
                softmax_vals[i] = s_softmax[ty * REG_TILE_SIZE + i][k];
            }
            
            float v_vals[REG_TILE_SIZE];
            #pragma unroll
            for (int j = 0; j < REG_TILE_SIZE; ++j) {
                v_vals[j] = s_V[tx * REG_TILE_SIZE + j][k];
            }
            
            #pragma unroll
            for (int i = 0; i < REG_TILE_SIZE; ++i) {
                #pragma unroll
                for (int j = 0; j < REG_TILE_SIZE; ++j) {
                    sum[i][j] += softmax_vals[i] * v_vals[j];
                }
            }
        }
        
        __syncthreads();
    }
    
    for (int i = 0; i < REG_TILE_SIZE; ++i) {
        int r = row + i;
        if (r < n) {
            for (int j = 0; j < REG_TILE_SIZE; ++j) {
                int c = col + j;
                if (c < n) {
                    Y[r * n + c] = sum[i][j];
                }
            }
        }
    }
}

void square_attention(int n, float* gpu_Q, float* gpu_K, float* gpu_V, float* gpu_Y) {
    float* gpu_QKT;
    cudaMalloc(&gpu_QKT, n * n * sizeof(float));
    
    float scale = 1.0f / sqrtf(n);
    
    dim3 block_dim(BLOCK_DIM, BLOCK_DIM);
    dim3 grid_dim((n + TILE_SIZE - 1) / TILE_SIZE, (n + TILE_SIZE - 1) / TILE_SIZE);
    
    matmul_QKT_register_blocked<<<grid_dim, block_dim>>>(n, gpu_Q, gpu_K, gpu_QKT, scale);
    
    int threads_per_block = 128;
    int warps_per_block = threads_per_block / WARP_SIZE;
    int num_blocks = (n + warps_per_block - 1) / warps_per_block;
    softmax_warp_optimized<<<num_blocks, threads_per_block>>>(n, gpu_QKT);
    
    matmul_softmax_V_register_blocked<<<grid_dim, block_dim>>>(n, gpu_QKT, gpu_V, gpu_Y);
    
    cudaFree(gpu_QKT);
    cudaDeviceSynchronize();
}
```









